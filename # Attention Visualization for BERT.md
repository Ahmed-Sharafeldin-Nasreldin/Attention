 # Attention Visualization for BERT

This code generates graphical representations of self-attention scores for a given text input using a pre-trained BERT model. The resulting diagrams provide insights into the model's attention patterns and can be useful for understanding how the model processes and interprets the input text.

## Prerequisites

To run this code, you will need the following:

- Python 3.6 or higher
- TensorFlow 2.0 or higher
- Transformers library
- Pillow library

## Installation

To install the required libraries, run the following commands in your terminal:

```
pip install tensorflow
pip install transformers
pip install pillow
```

## Usage

To use the code, follow these steps:

1. Clone the repository to your local machine.
2. Open a terminal and navigate to the directory where the code is saved.
3. Run the following command to start the script:

```
python main.py
```

You will be prompted to enter a text input. The script will then generate attention diagrams for the input text and save them in the `Attention_Layers` folder.

## Code Overview

The code is organized into the following main functions:

- `main()`: This function is the entry point of the script. It prompts the user for input text, loads the pre-trained BERT model, and generates attention diagrams.
- `get_mask_token_index()`: This function returns the index of the mask token in the input text.
- `get_color_for_attention_score()`: This function converts an attention score to a tuple of three integers representing a shade of gray.
- `visualize_attentions()`: This function generates a graphical representation of self-attention scores for a single attention head.
- `generate_diagram()`: This function generates a diagram representing the self-attention scores for a single attention head.

## Understanding the Code

### Loading the BERT Model

The code loads the pre-trained BERT model using the `TFBertForMaskedLM` class from the Transformers library. The model is loaded with the `MODEL` constant, which is set to `"bert-base-uncased"` by default.

```python
model = TFBertForMaskedLM.from_pretrained(MODEL)
```

### Generating Attention Diagrams

The code generates attention diagrams for the input text using the `visualize_attentions()` function. This function iterates through all layers and heads in the BERT model and generates

Generated by [BlackboxAI](https://www.blackbox.ai)